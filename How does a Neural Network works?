Okay, it will be hard to explain without using images.
Lets see what i remember.

Lets say we have 2 input layers and 2 output layers

Name inputs as I1 and I2 and outputs O1 and O2.

I1 and I2 is connected to O1 with weights ( numeralize them too) such as W1.1 and W2.1 and connected to O2 with W1.2 and W2.2.


What are weights? Imagine them as [ W(I) = I x w ] so its a multiplier.

so if I1 is 1, and W1.1 is 1, O1 is 1 + W(I2) and if I2 is 3 and W2.1 is -1 O1 will be -2.

Now, lets add a layer.

I1,I2 goes through L1 to O1 and O2.

Like before, it has weights attached to each of the layers.

But this time layer in the middle also have biases [ F(x) = aX^2 + bx - c ]. Bias is C value and each neuron has their own biases.

We have biases, so we can shift the neurons.

If F(x) was ax^2+bx then F(0) would be always be 0. We dont want that thats why we have biases.


After F(x) we have activation functions. These functions simulate neuron fire.

Activation(F(x)) 

Some common activation functions:
1-sigmoid = f(x) = 1 / (1 + exp(-x)), used for binary classification 
2-ReLU = f(x) = max(0,x) used for deep learning
3-tanH f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)) limited range, -1 and 1
4-SoftMax f(x) = exp(x) / sum(exp(x)) used for output layer of probability for multi class

Then we move onto Loss functions (A.K.A. cost function or error function), Loss functions are the functions that give us the difference between "desired" output and the output the current algorithm gives. We would want to decrease the value of loss function, each generation so the algorithm is optimized for our usage.

Here are some loss functions:
Mean Squared Error (MSE) ( This is commonly used for regression problems, where the goal is predicting a continious value.  It measures the average of the squared differences between the predicted and true values)
Binary Cross-Entropy ( this is commonly used for binary classification problems (0 or 1) )
Categorical Cross-Entropy (This is commonly used for multi-class classification problems)
Mean Absolute Error (This is commonly used for regression problems, It measures the average of the absolute differences between the predicted and true values)

```
import numpy as np

# define the true and predicted values
y_true = np.array([1.0, 2.0, 3.0, 4.0])
y_pred = np.array([1.2, 2.3, 3.1, 3.9])

# calculate the mean squared error
mse = np.mean((y_pred - y_true) ** 2)

print("Mean squared error:", mse)
```

Good and all, but how do we actually optimize the weight and biases so we have better output?

Time to introduce backpropagation. (https://www.youtube.com/watch?v=Ilg3gGewQ5U)



